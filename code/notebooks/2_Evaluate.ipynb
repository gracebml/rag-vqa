{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Evaluate tr√™n t·∫≠p Test\n",
        "\n",
        "Notebook n√†y ch·∫°y ƒë√°nh gi√° h·ªá th·ªëng tr√™n t·∫≠p test v√† t√≠nh c√°c metrics:\n",
        "- **BERTScore**: Semantic similarity\n",
        "- **LLM-as-a-Judge**: Expert scoring v·ªõi Gemini API\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q transformers accelerate bitsandbytes qwen-vl-utils sentence-transformers rank-bm25 underthesea wikipedia pillow pandas tqdm\n",
        "%pip install -q bert-score google-generativeai matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import th∆∞ vi·ªán\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Add src to path (if running in Kaggle)\n",
        "sys.path.insert(0, '/kaggle/working/code/src' if Path('/kaggle/working').exists() else '../src')\n",
        "\n",
        "from pipeline import RAGVQAPipeline\n",
        "from evaluation_metrics import VQAEvaluator \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. C·∫•u h√¨nh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "TEST_DATA_PATH = \"/kaggle/input/vqa-test/vqa_test.json\"  \n",
        "IMAGES_DIR = \"/kaggle/input/vqa-images/images_flat\"  \n",
        "KB_PATH = \"/kaggle/input/vietnamese-knowledge-base/knowledge_base.json\"\n",
        "\n",
        "# Output\n",
        "OUTPUT_DIR = \"/kaggle/working/results\"\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Evaluation settings\n",
        "MAX_SAMPLES = None  # Set to number to limit, None for all\n",
        "BATCH_SIZE = 1  # Process one at a time\n",
        "\n",
        "\n",
        "# EVALUATION METRICS CONFIG \n",
        "\n",
        "USE_BERT_SCORE = True\n",
        "USE_LLM_JUDGE = True  # Set to False if no Gemini API key\n",
        "\n",
        "# Set your Gemini API key here or via environment variable\n",
        "# Get free API key at: https://makersuite.google.com/app/apikey\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", None)  # ‚Üê Set your key here or use env var\n",
        "\n",
        "# If you don't have API key, disable LLM Judge:\n",
        "if GEMINI_API_KEY is None and USE_LLM_JUDGE:\n",
        "    print(\"‚ö†Ô∏è  WARNING: GEMINI_API_KEY not set. LLM Judge will be disabled.\")\n",
        "    print(\"Get free API key at: https://makersuite.google.com/app/apikey\")\n",
        "    USE_LLM_JUDGE = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading test data...\")\n",
        "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    test_data = test_data[:MAX_SAMPLES]\n",
        "\n",
        "print(f\"Loaded {len(test_data)} test samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Initialize Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Initializing VQA pipeline...\")\n",
        "pipeline = RAGVQAPipeline(use_4bit=True)\n",
        "print(\"‚úì Pipeline ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run Predictions\n",
        "\n",
        "Generate predictions for all test samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING PREDICTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, item in enumerate(tqdm(test_data, desc=\"Processing\")):\n",
        "    try:\n",
        "        # Load image\n",
        "        image_path = Path(IMAGES_DIR) / Path(item['image_path']).name\n",
        "        if not image_path.exists():\n",
        "            print(f\"Image not found: {image_path}\")\n",
        "            continue\n",
        "        \n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        question = item['question']\n",
        "        ground_truth = item.get('answer', {}).get('answer', '')\n",
        "        \n",
        "        # Get prediction\n",
        "        result = pipeline.process(\n",
        "            image=image,\n",
        "            question=question,\n",
        "            return_intermediate=True\n",
        "        )\n",
        "        \n",
        "        predictions.append({\n",
        "            'image_id': item.get('image_id', f'img_{i:06d}'),\n",
        "            'question': question,\n",
        "            'prediction': result['answer'],\n",
        "            'ground_truth': ground_truth,\n",
        "            'caption': result.get('caption', ''),\n",
        "            'ocr': result.get('ocr', ''),\n",
        "            'num_retrieved': len(result.get('retrieved_docs', []))\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sample {i}: {e}\")\n",
        "        predictions.append({\n",
        "            'image_id': item.get('image_id', f'img_{i:06d}'),\n",
        "            'question': question,\n",
        "            'prediction': f'ERROR: {str(e)}',\n",
        "            'ground_truth': ground_truth,\n",
        "            'caption': '',\n",
        "            'ocr': '',\n",
        "            'num_retrieved': 0\n",
        "        })\n",
        "\n",
        "print(f\"\\n‚úì Completed predictions on {len(predictions)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions (needed for evaluation)\n",
        "predictions_file = f\"{OUTPUT_DIR}/predictions.json\"\n",
        "with open(predictions_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
        "print(f\"‚úì Predictions saved to {predictions_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run Evaluation Metrics\n",
        "\n",
        "\n",
        "ƒê√°nh gi√° predictions b·∫±ng:\n",
        "1. **BERTScore** - Semantic similarity\n",
        "2. **LLM-as-a-Judge** - Expert scoring v·ªõi Gemini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATING WITH METRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = VQAEvaluator(\n",
        "    use_bert_score=USE_BERT_SCORE,\n",
        "    use_llm_judge=USE_LLM_JUDGE,\n",
        "    gemini_api_key=GEMINI_API_KEY,\n",
        "    bert_model=\"bert-base-multilingual-cased\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_output = f\"{OUTPUT_DIR}/evaluation_results.json\"\n",
        "stats = evaluator.evaluate_dataset(\n",
        "    predictions_file=predictions_file,\n",
        "    output_file=evaluation_output,\n",
        "    llm_judge_delay=1.5  # Delay to avoid rate limit\n",
        ")\n",
        "\n",
        "print(\"\\n‚úì Evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Display Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if USE_BERT_SCORE:\n",
        "    print(\"\\nüìä BERTScore Results:\")\n",
        "    bert_stats = stats.get('bert_score', {})\n",
        "    print(f\"  Mean:   {bert_stats.get('mean', 0):.4f}\")\n",
        "    print(f\"  Std:    {bert_stats.get('std', 0):.4f}\")\n",
        "    print(f\"  Min:    {bert_stats.get('min', 0):.4f}\")\n",
        "    print(f\"  Max:    {bert_stats.get('max', 0):.4f}\")\n",
        "    print(f\"  Median: {bert_stats.get('median', 0):.4f}\")\n",
        "\n",
        "if USE_LLM_JUDGE:\n",
        "    print(\"\\nüìä LLM Judge Results:\")\n",
        "    llm_stats = stats.get('llm_judge', {})\n",
        "    print(f\"  Mean:   {llm_stats.get('mean', 0):.2f}/5\")\n",
        "    print(f\"  Std:    {llm_stats.get('std', 0):.2f}\")\n",
        "    print(f\"  Min:    {llm_stats.get('min', 0):.2f}/5\")\n",
        "    print(f\"  Max:    {llm_stats.get('max', 0):.2f}/5\")\n",
        "    print(f\"  Median: {llm_stats.get('median', 0):.2f}/5\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Compare with Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüìà Comparison with Baseline (from report):\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"{'Method':<30} {'BERTScore (%)':<20} {'LLM Judge (1-5)'}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Baseline (Zero-shot)':<30} {'42.1':<20} {'3.5'}\")\n",
        "\n",
        "if USE_BERT_SCORE and USE_LLM_JUDGE:\n",
        "    bert_pct = stats['bert_score']['mean'] * 100\n",
        "    llm_score = stats['llm_judge']['mean']\n",
        "    print(f\"{'Your Result (RAG)':<30} {bert_pct:<20.1f} {llm_score:.1f}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Check if better than baseline\n",
        "    if bert_pct > 42.1 and llm_score > 3.5:\n",
        "        print(\"\\n‚úÖ Your result is BETTER than baseline! Great job!\")\n",
        "    elif bert_pct > 42.1 or llm_score > 3.5:\n",
        "        print(\"\\n‚ö†Ô∏è  Your result is partially better than baseline.\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Your result needs improvement.\")\n",
        "elif USE_BERT_SCORE:\n",
        "    bert_pct = stats['bert_score']['mean'] * 100\n",
        "    print(f\"{'Your Result (RAG)':<30} {bert_pct:<20.1f} {'N/A (no LLM Judge)'}\")\n",
        "    print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load evaluation results\n",
        "with open(evaluation_output, 'r', encoding='utf-8') as f:\n",
        "    eval_data = json.load(f)\n",
        "\n",
        "results_list = eval_data['results']\n",
        "\n",
        "# Extract scores\n",
        "bert_scores = [r['bert_score'] for r in results_list if r.get('bert_score') is not None]\n",
        "llm_scores = [r['llm_judge_score'] for r in results_list if r.get('llm_judge_score') is not None]\n",
        "\n",
        "# Create plots\n",
        "fig, axes = plt.subplots(1, 2 if USE_LLM_JUDGE else 1, figsize=(14 if USE_LLM_JUDGE else 7, 5))\n",
        "\n",
        "if USE_BERT_SCORE:\n",
        "    ax = axes[0] if USE_LLM_JUDGE else axes\n",
        "    ax.hist(bert_scores, bins=20, color='skyblue', edgecolor='black')\n",
        "    ax.axvline(stats['bert_score']['mean'], color='red', linestyle='--', \n",
        "               label=f\"Mean: {stats['bert_score']['mean']:.4f}\")\n",
        "    ax.set_xlabel('BERTScore')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title('BERTScore Distribution')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "if USE_LLM_JUDGE:\n",
        "    ax = axes[1] if USE_BERT_SCORE else axes\n",
        "    ax.hist(llm_scores, bins=5, color='lightgreen', edgecolor='black', range=(1, 5))\n",
        "    ax.axvline(stats['llm_judge']['mean'], color='red', linestyle='--',\n",
        "               label=f\"Mean: {stats['llm_judge']['mean']:.2f}\")\n",
        "    ax.set_xlabel('LLM Judge Score')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title('LLM Judge Score Distribution')\n",
        "    ax.set_xticks([1, 2, 3, 4, 5])\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "viz_path = f\"{OUTPUT_DIR}/evaluation_distributions.png\"\n",
        "plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Visualization saved to {viz_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Sample Detailed Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few detailed results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE DETAILED RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, result in enumerate(results_list[:3]):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Sample {i+1}: {result['image_id']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\n‚ùì Question: {result['question']}\")\n",
        "    print(f\"\\n‚úÖ Ground Truth: {result['ground_truth']}\")\n",
        "    print(f\"\\nü§ñ Prediction: {result['predicted_answer']}\")\n",
        "    \n",
        "    if result.get('bert_score'):\n",
        "        print(f\"\\nüìä BERTScore: {result['bert_score']:.4f}\")\n",
        "    \n",
        "    if result.get('llm_judge_score'):\n",
        "        print(f\"üìä LLM Judge: {result['llm_judge_score']}/5\")\n",
        "        print(f\"üí≠ Reasoning: {result.get('llm_judge_reasoning', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Export to CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame for easy viewing\n",
        "df = pd.DataFrame(results_list)\n",
        "\n",
        "# Save to CSV\n",
        "csv_path = f\"{OUTPUT_DIR}/evaluation_results.csv\"\n",
        "df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n‚úì Results exported to {csv_path}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total samples evaluated: {len(df)}\")\n",
        "print(f\"Samples with caption: {(df['caption'] != '').sum()}\")\n",
        "print(f\"Samples with OCR: {(df['ocr'] != '').sum()}\")\n",
        "print(f\"Average retrieved docs: {df.get('num_retrieved', pd.Series([0])).mean():.2f}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. K·∫øt lu·∫≠n\n",
        "\n",
        "\n",
        "\n",
        "###  Files ƒë∆∞·ª£c t·∫°o:\n",
        "1. `predictions.json` - Raw predictions\n",
        "2. `evaluation_results.json` - Detailed evaluation results\n",
        "3. `evaluation_results.csv` - CSV format for easy viewing\n",
        "4. `evaluation_distributions.png` - Visualizations\n",
        "\n",
        "### üéØ K·∫øt qu·∫£ mong ƒë·ª£i:\n",
        "- BERTScore > 42.1%\n",
        "- LLM Judge > 3.5/5\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
