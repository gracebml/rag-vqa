{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Evaluate trên tập Test\n",
        "\n",
        "Notebook này chạy đánh giá hệ thống trên tập test và tính các metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Cài đặt thư viện\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q transformers accelerate bitsandbytes qwen-vl-utils sentence-transformers rank-bm25 underthesea wikipedia pillow pandas tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import thư viện\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Add src to path (if running in Kaggle)\n",
        "sys.path.insert(0, '/kaggle/working/code/src' if Path('/kaggle/working').exists() else '../src')\n",
        "\n",
        "from pipeline import RAGVQAPipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cấu hình\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "TEST_DATA_PATH = \"/kaggle/input/vqa-test/vqa_test.json\"  # Điều chỉnh theo dataset của bạn\n",
        "IMAGES_DIR = \"/kaggle/input/vqa-images/images_flat\"  # Điều chỉnh theo dataset của bạn\n",
        "KB_PATH = \"/kaggle/input/vietnamese-knowledge-base/knowledge_base.json\"\n",
        "\n",
        "# Output\n",
        "OUTPUT_DIR = \"/kaggle/working/results\"\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Evaluation settings\n",
        "MAX_SAMPLES = None  # Set to number to limit, None for all\n",
        "BATCH_SIZE = 1  # Process one at a time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading test data...\")\n",
        "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    test_data = test_data[:MAX_SAMPLES]\n",
        "\n",
        "print(f\"Loaded {len(test_data)} test samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Initialize Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Initializing pipeline...\")\n",
        "pipeline = RAGVQAPipeline(use_4bit=True)\n",
        "print(\"Pipeline ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for i, item in enumerate(tqdm(test_data, desc=\"Evaluating\")):\n",
        "    try:\n",
        "        # Load image\n",
        "        image_path = Path(IMAGES_DIR) / Path(item['image_path']).name\n",
        "        if not image_path.exists():\n",
        "            print(f\"Image not found: {image_path}\")\n",
        "            continue\n",
        "        \n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        question = item['question']\n",
        "        ground_truth = item.get('answer', {}).get('answer', '')\n",
        "        \n",
        "        # Get prediction\n",
        "        result = pipeline.process(\n",
        "            image=image,\n",
        "            question=question,\n",
        "            return_intermediate=True\n",
        "        )\n",
        "        \n",
        "        results.append({\n",
        "            'id': i,\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'prediction': result['answer'],\n",
        "            'caption': result.get('caption', ''),\n",
        "            'ocr': result.get('ocr', ''),\n",
        "            'num_retrieved': len(result.get('retrieved_docs', []))\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sample {i}: {e}\")\n",
        "        results.append({\n",
        "            'id': i,\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'prediction': f'ERROR: {str(e)}',\n",
        "            'caption': '',\n",
        "            'ocr': '',\n",
        "            'num_retrieved': 0\n",
        "        })\n",
        "\n",
        "print(f\"\\nCompleted evaluation on {len(results)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to JSON\n",
        "output_json = f\"{OUTPUT_DIR}/evaluation_results.json\"\n",
        "with open(output_json, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "print(f\"Results saved to {output_json}\")\n",
        "\n",
        "# Save to CSV for easy viewing\n",
        "df = pd.DataFrame(results)\n",
        "output_csv = f\"{OUTPUT_DIR}/evaluation_results.csv\"\n",
        "df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
        "print(f\"Results saved to {output_csv}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n=== Evaluation Summary ===\")\n",
        "print(f\"Total samples: {len(results)}\")\n",
        "print(f\"Average retrieved docs: {df['num_retrieved'].mean():.2f}\")\n",
        "print(f\"Samples with caption: {(df['caption'] != '').sum()}\")\n",
        "print(f\"Samples with OCR: {(df['ocr'] != '').sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Sample Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few results\n",
        "for i, result in enumerate(results[:3]):\n",
        "    print(f\"\\n=== Sample {i+1} ===\")\n",
        "    print(f\"Question: {result['question']}\")\n",
        "    print(f\"Ground Truth: {result['ground_truth'][:100]}...\")\n",
        "    print(f\"Prediction: {result['prediction'][:100]}...\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
