{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Evaluation Metrics cho Vietnamese Cultural VQA\n",
    "\n",
    "\n",
    "\n",
    "Notebook n√†y demo c√°ch s·ª≠ d·ª•ng 2 metrics ƒë√°nh gi√°:\n",
    "1. **BERTScore** - Semantic similarity\n",
    "2. **LLM-as-a-Judge** - Expert scoring v·ªõi Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment n·∫øu ch∆∞a c√†i)\n",
    "# !pip install bert-score google-generativeai numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from evaluation_metrics import (\n",
    "    BERTScoreEvaluator,\n",
    "    LLMJudgeEvaluator,\n",
    "    VQAEvaluator\n",
    ")\n",
    "\n",
    "# Set API key (QUAN TR·ªåNG!)\n",
    "# L·∫•y API key mi·ªÖn ph√≠ t·∫°i: https://makersuite.google.com/app/apikey\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"YOUR_API_KEY_HERE\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Demo BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTScore evaluator\n",
    "bert_evaluator = BERTScoreEvaluator(\n",
    "    model_name=\"bert-base-multilingual-cased\",\n",
    "    device=\"cuda\"  # ƒê·ªïi sang \"cpu\" n·∫øu kh√¥ng c√≥ GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case 1: C√¢u tr·∫£ l·ªùi g·∫ßn gi·ªëng\n",
    "prediction1 = \"Ch√πa M·ªôt C·ªôt ƒë∆∞·ª£c x√¢y d·ª±ng v√†o nƒÉm 1049 d∆∞·ªõi tri·ªÅu L√Ω.\"\n",
    "ground_truth1 = \"Ch√πa M·ªôt C·ªôt, tri·ªÅu L√Ω, nƒÉm 1049.\"\n",
    "\n",
    "score1 = bert_evaluator.evaluate_single(prediction1, ground_truth1)\n",
    "print(f\"Test 1 - C√¢u tr·∫£ l·ªùi t·ªët:\")\n",
    "print(f\"  Prediction: {prediction1}\")\n",
    "print(f\"  Ground Truth: {ground_truth1}\")\n",
    "print(f\"  BERTScore: {score1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case 2: C√¢u tr·∫£ l·ªùi sai\n",
    "prediction2 = \"ƒê√¢y l√† VƒÉn Mi·∫øu, ƒë∆∞·ª£c x√¢y d·ª±ng nƒÉm 1070.\"\n",
    "ground_truth2 = \"Ch√πa M·ªôt C·ªôt, tri·ªÅu L√Ω, nƒÉm 1049.\"\n",
    "\n",
    "score2 = bert_evaluator.evaluate_single(prediction2, ground_truth2)\n",
    "print(f\"Test 2 - C√¢u tr·∫£ l·ªùi sai:\")\n",
    "print(f\"  Prediction: {prediction2}\")\n",
    "print(f\"  Ground Truth: {ground_truth2}\")\n",
    "print(f\"  BERTScore: {score2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch evaluation\n",
    "predictions = [\n",
    "    \"V·ªãnh H·∫° Long l√† di s·∫£n thi√™n nhi√™n th·∫ø gi·ªõi\",\n",
    "    \"Ph·ªë c·ªï H·ªôi An ƒë∆∞·ª£c UNESCO c√¥ng nh·∫≠n nƒÉm 1999\",\n",
    "    \"VƒÉn Mi·∫øu l√† tr∆∞·ªùng ƒë·∫°i h·ªçc ƒë·∫ßu ti√™n c·ªßa Vi·ªát Nam\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"V·ªãnh H·∫° Long ƒë∆∞·ª£c UNESCO c√¥ng nh·∫≠n l√† di s·∫£n th·∫ø gi·ªõi nƒÉm 1994\",\n",
    "    \"H·ªôi An l√† di s·∫£n vƒÉn h√≥a th·∫ø gi·ªõi t·ª´ nƒÉm 1999\",\n",
    "    \"VƒÉn Mi·∫øu - Qu·ªëc T·ª≠ Gi√°m, tr∆∞·ªùng ƒë·∫°i h·ªçc ƒë·∫ßu ti√™n VN, th√†nh l·∫≠p 1076\"\n",
    "]\n",
    "\n",
    "scores = bert_evaluator.evaluate_batch(predictions, references)\n",
    "stats = bert_evaluator.get_statistics(scores)\n",
    "\n",
    "print(\"Batch Evaluation Results:\")\n",
    "for i, (pred, ref, score) in enumerate(zip(predictions, references, scores), 1):\n",
    "    print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Pred: {pred}\")\n",
    "    print(f\"   Ref:  {ref}\")\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Mean: {stats['mean']:.4f}\")\n",
    "print(f\"  Std:  {stats['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demo LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM Judge\n",
    "llm_evaluator = LLMJudgeEvaluator(\n",
    "    api_key=None,  # Will use GEMINI_API_KEY from environment\n",
    "    model_name=\"gemini-1.5-flash\"  # Free tier model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case 1: C√¢u tr·∫£ l·ªùi xu·∫•t s·∫Øc\n",
    "question1 = \"ƒê·ªãa ƒëi·ªÉm trong ·∫£nh c√≥ ph·∫£i di s·∫£n thi√™n nhi√™n th·∫ø gi·ªõi kh√¥ng?\"\n",
    "prediction1 = \"C√≥, ƒë√¢y l√† V·ªãnh H·∫° Long, m·ªôt di s·∫£n thi√™n nhi√™n th·∫ø gi·ªõi ƒë∆∞·ª£c UNESCO c√¥ng nh·∫≠n nƒÉm 1994.\"\n",
    "ground_truth1 = \"C√≥, V·ªãnh H·∫° Long l√† di s·∫£n thi√™n nhi√™n th·∫ø gi·ªõi, ƒë∆∞·ª£c UNESCO c√¥ng nh·∫≠n nƒÉm 1994.\"\n",
    "\n",
    "score1, reasoning1 = llm_evaluator.evaluate_single(question1, prediction1, ground_truth1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Test 1: C√¢u tr·∫£ l·ªùi xu·∫•t s·∫Øc\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"C√¢u h·ªèi: {question1}\")\n",
    "print(f\"\\nD·ª± ƒëo√°n: {prediction1}\")\n",
    "print(f\"\\nGround Truth: {ground_truth1}\")\n",
    "print(f\"\\nüìä ƒêi·ªÉm: {score1}/5\")\n",
    "print(f\"üí≠ L√Ω do: {reasoning1}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case 2: C√¢u tr·∫£ l·ªùi c√≥ hallucination\n",
    "question2 = \"Ch√πa M·ªôt C·ªôt ƒë∆∞·ª£c x√¢y d·ª±ng v√†o th·ªùi n√†o?\"\n",
    "prediction2 = \"Ch√πa M·ªôt C·ªôt ƒë∆∞·ª£c x√¢y d·ª±ng v√†o nƒÉm 1076 d∆∞·ªõi th·ªùi vua L√Ω Nh√¢n T√¥ng.\"\n",
    "ground_truth2 = \"Ch√πa M·ªôt C·ªôt ƒë∆∞·ª£c x√¢y d·ª±ng nƒÉm 1049 d∆∞·ªõi th·ªùi vua L√Ω Th√°i T√¥ng.\"\n",
    "\n",
    "score2, reasoning2 = llm_evaluator.evaluate_single(question2, prediction2, ground_truth2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Test 2: C√¢u tr·∫£ l·ªùi c√≥ sai s√≥t (hallucination)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"C√¢u h·ªèi: {question2}\")\n",
    "print(f\"\\nD·ª± ƒëo√°n: {prediction2}\")\n",
    "print(f\"\\nGround Truth: {ground_truth2}\")\n",
    "print(f\"\\nüìä ƒêi·ªÉm: {score2}/5\")\n",
    "print(f\"üí≠ L√Ω do: {reasoning2}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demo Combined Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize combined evaluator\n",
    "evaluator = VQAEvaluator(\n",
    "    use_bert_score=True,\n",
    "    use_llm_judge=True,\n",
    "    gemini_api_key=None,  # Use env var\n",
    "    bert_model=\"bert-base-multilingual-cased\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate single sample\n",
    "result = evaluator.evaluate_single(\n",
    "    image_id=\"001234\",\n",
    "    question=\"ƒê√¢y l√† di t√≠ch g√¨?\",\n",
    "    prediction=\"ƒê√¢y l√† VƒÉn Mi·∫øu - Qu·ªëc T·ª≠ Gi√°m, ƒë∆∞·ª£c x√¢y d·ª±ng nƒÉm 1070.\",\n",
    "    ground_truth=\"VƒÉn Mi·∫øu - Qu·ªëc T·ª≠ Gi√°m, th√†nh l·∫≠p nƒÉm 1070 d∆∞·ªõi tri·ªÅu L√Ω.\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Image ID: {result.image_id}\")\n",
    "print(f\"Question: {result.question}\")\n",
    "print(f\"\\nPredicted: {result.predicted_answer}\")\n",
    "print(f\"\\nGround Truth: {result.ground_truth}\")\n",
    "print(f\"\\nüìä Metrics:\")\n",
    "print(f\"  - BERTScore: {result.bert_score:.4f}\")\n",
    "print(f\"  - LLM Judge: {result.llm_judge_score}/5\")\n",
    "print(f\"\\nüí≠ LLM Reasoning: {result.llm_judge_reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Full Dataset\n",
    "\n",
    "ƒê·ªÉ ch·∫°y tr√™n to√†n b·ªô dataset, c·∫ßn chu·∫©n b·ªã file JSON v·ªõi format:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"image_id\": \"001234\",\n",
    "    \"question\": \"...\",\n",
    "    \"prediction\": \"...\",\n",
    "    \"ground_truth\": \"...\"\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample predictions file for demo\n",
    "sample_data = [\n",
    "    {\n",
    "        \"image_id\": \"001234\",\n",
    "        \"question\": \"ƒê√¢y l√† di t√≠ch g√¨?\",\n",
    "        \"prediction\": \"Ch√πa M·ªôt C·ªôt ƒë∆∞·ª£c x√¢y d·ª±ng nƒÉm 1049 d∆∞·ªõi tri·ªÅu L√Ω.\",\n",
    "        \"ground_truth\": \"Ch√πa M·ªôt C·ªôt, tri·ªÅu L√Ω, nƒÉm 1049.\"\n",
    "    },\n",
    "    {\n",
    "        \"image_id\": \"001235\",\n",
    "        \"question\": \"ƒê·ªãa ƒëi·ªÉm n√†y c√≥ ph·∫£i di s·∫£n th·∫ø gi·ªõi kh√¥ng?\",\n",
    "        \"prediction\": \"C√≥, V·ªãnh H·∫° Long l√† di s·∫£n thi√™n nhi√™n th·∫ø gi·ªõi.\",\n",
    "        \"ground_truth\": \"C√≥, ƒë∆∞·ª£c UNESCO c√¥ng nh·∫≠n nƒÉm 1994.\"\n",
    "    },\n",
    "    {\n",
    "        \"image_id\": \"001236\",\n",
    "        \"question\": \"VƒÉn Mi·∫øu ƒë∆∞·ª£c x√¢y d·ª±ng khi n√†o?\",\n",
    "        \"prediction\": \"VƒÉn Mi·∫øu ƒë∆∞·ª£c th√†nh l·∫≠p nƒÉm 1070.\",\n",
    "        \"ground_truth\": \"VƒÉn Mi·∫øu - Qu·ªëc T·ª≠ Gi√°m th√†nh l·∫≠p nƒÉm 1070 d∆∞·ªõi tri·ªÅu L√Ω.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to file\n",
    "with open(\"sample_predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úì Created sample_predictions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate entire dataset\n",
    "stats = evaluator.evaluate_dataset(\n",
    "    predictions_file=\"sample_predictions.json\",\n",
    "    output_file=\"evaluation_results.json\",\n",
    "    llm_judge_delay=2.0  # 2 seconds delay to avoid rate limit\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(stats, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load results\n",
    "with open(\"evaluation_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results_data = json.load(f)\n",
    "\n",
    "# Extract scores\n",
    "bert_scores = [r[\"bert_score\"] for r in results_data[\"results\"] if r[\"bert_score\"] is not None]\n",
    "llm_scores = [r[\"llm_judge_score\"] for r in results_data[\"results\"] if r[\"llm_judge_score\"] is not None]\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# BERTScore distribution\n",
    "axes[0].hist(bert_scores, bins=20, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(results_data[\"statistics\"][\"bert_score\"][\"mean\"], \n",
    "                color='red', linestyle='--', label=f'Mean: {results_data[\"statistics\"][\"bert_score\"][\"mean\"]:.4f}')\n",
    "axes[0].set_xlabel('BERTScore')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('BERTScore Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# LLM Judge distribution\n",
    "axes[1].hist(llm_scores, bins=5, color='lightgreen', edgecolor='black', range=(1, 5))\n",
    "axes[1].axvline(results_data[\"statistics\"][\"llm_judge\"][\"mean\"], \n",
    "                color='red', linestyle='--', label=f'Mean: {results_data[\"statistics\"][\"llm_judge\"][\"mean\"]:.2f}')\n",
    "axes[1].set_xlabel('LLM Judge Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('LLM Judge Score Distribution')\n",
    "axes[1].set_xticks([1, 2, 3, 4, 5])\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved visualization to evaluation_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between BERTScore and LLM Judge\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(bert_scores, llm_scores, alpha=0.6)\n",
    "plt.xlabel('BERTScore')\n",
    "plt.ylabel('LLM Judge Score')\n",
    "plt.title('Correlation: BERTScore vs LLM Judge')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "correlation = np.corrcoef(bert_scores, llm_scores)[0, 1]\n",
    "plt.text(0.05, 4.8, f'Correlation: {correlation:.3f}', \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.savefig('score_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved correlation plot to score_correlation.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
